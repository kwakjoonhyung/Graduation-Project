{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f029753a-c9ba-4f63-a28e-83b474bb9e38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d06c139-a2f2-4267-8dcb-5dae0e22ecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17142bd0-2885-4f0f-9657-b215f8127a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"4\"  # Set the GPU 5 to use\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device =='cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc884db6-a408-49a0-86d0-3aeb53f1a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA GeForce RTX 3080\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print('cuda index:', torch.cuda.current_device())\n",
    "\n",
    "print('gpu 개수:', torch.cuda.device_count())\n",
    "\n",
    "print('graphic name:', torch.cuda.get_device_name())\n",
    "\n",
    "cuda = torch.device('cuda')\n",
    "\n",
    "print(cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40918939-5c7d-40ac-a018-d80f469d4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') #GPU 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbdb7e21-cbc9-4810-8e69-fe7518af293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afda040-521f-4808-a90e-cefdcd4bcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(929)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d06a67e-f782-45c5-8ac6-b6a554a8c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"강제추행(성범죄)\": 1,\n",
    "    \"강도범죄\": 2,\n",
    "    \"절도범죄\": 3,\n",
    "    \"폭력범죄\": 4,\n",
    "    \"도움요청\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76734e40-b8fe-4793-abc0-aafc6aba7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset():\n",
    "    folder = \"D:\\\\위험상황데이터셋\\\\dataset\\\\train\"\n",
    "    dataset = []\n",
    "    class_label = 0\n",
    "    for file in tqdm(os.listdir(folder),colour='green'):\n",
    "        if 'wav' in file:\n",
    "            abs_file_path = os.path.join(folder,file)\n",
    "            data, sr = librosa.load(abs_file_path, sr = 16000,mono=True)\n",
    "            \n",
    "            json_path = abs_file_path.replace(\"_label.wav\",\".json\")\n",
    "            with open(json_path,encoding='utf-8') as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                categories = json_data[\"annotations\"][0][\"categories\"]\n",
    "                category = categories[\"category_02\"]\n",
    "                class_label = int(label_dict[category])\n",
    "            dataset.append([data,class_label])\n",
    "    \n",
    "    print(\"Dataset 생성 완료\")\n",
    "    return pd.DataFrame(dataset,columns=['data','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64612e04-e4bb-45af-b5e1-29f59b0899f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    folder = \"D:\\\\위험상황데이터셋\\\\dataset\\\\test\"\n",
    "    dataset = []\n",
    "    for file in tqdm(os.listdir(folder),colour='green'):\n",
    "        if 'wav' in file:\n",
    "            abs_file_path = os.path.join(folder,file)\n",
    "            data, sr = librosa.load(abs_file_path, sr = 16000,mono=True)\n",
    "            \n",
    "            dataset.append([data, file])\n",
    "    \n",
    "    print(\"Dataset 생성 완료\")\n",
    "    return pd.DataFrame(dataset,columns=['data', 'file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23b8d54-1777-4ff6-9679-ef7750b02a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m█████████████████████████████████████████████████████████\u001b[0m| 22000/22000 [00:52<00:00, 417.95it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m███████████████████████████████████████████████████████████\u001b[0m| 5000/5000 [00:12<00:00, 386.15it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 생성 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_wav = train_dataset()\n",
    "test_wav = test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "204e2c54-4765-4fba-824c-c95aa7da97f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-9.62628e-05, -0.00020544285, -0.00011758789,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.169485e-05, 2.5358913e-05, 3.8124912e-05, 4...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00013293176, 0.0001595899, 1.5221034e-05, -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-4.965741e-05, -4.61519e-05, 2.123904e-05, 5....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00029593214, -0.0006093591, -0.0005188418,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  label\n",
       "0  [-9.62628e-05, -0.00020544285, -0.00011758789,...      1\n",
       "1  [1.169485e-05, 2.5358913e-05, 3.8124912e-05, 4...      1\n",
       "2  [0.00013293176, 0.0001595899, 1.5221034e-05, -...      1\n",
       "3  [-4.965741e-05, -4.61519e-05, 2.123904e-05, 5....      1\n",
       "4  [-0.00029593214, -0.0006093591, -0.0005188418,...      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_wav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dadf1a1-b4a2-4d31-a91c-c86bf7c7ccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    2207\n",
      "5    2200\n",
      "3    2200\n",
      "4    2200\n",
      "1    2193\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = train_wav['label'].value_counts()\n",
    "\n",
    "# 결과 출력\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d96761-93c7-4ee9-a854-87790c7b2aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "train_x = np.array(train_wav.data)\n",
    "test_x = np.array(test_wav.data)\n",
    "data_type = type(train_x)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ebbe01-ab6f-406d-bdad-f9e89669c93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x length Histogram\n",
      "[  10240.    92864.2  175488.4  258112.6  340736.8  423361.   505985.2\n",
      "  588609.4  671233.6  753857.8  836482.   919106.2 1001730.4 1084354.6\n",
      " 1166978.8 1249603. ]\n",
      "[9417 1079  385  102    0    0    1    0    7    3    0    0    0    0\n",
      "    6]\n",
      "test_x length Histogram\n",
      "[   7680.           90453.53333333  173227.06666667  256000.6\n",
      "  338774.13333333  421547.66666667  504321.2         587094.73333333\n",
      "  669868.26666667  752641.8         835415.33333333  918188.86666667\n",
      " 1000962.4        1083735.93333333 1166509.46666667 1249283.        ]\n",
      "[2022  267  150   51    0    0    1    0    4    2    0    0    0    0\n",
      "    3]\n"
     ]
    }
   ],
   "source": [
    "train_length=[]\n",
    "test_length=[]\n",
    "\n",
    "for i in train_x:\n",
    "    train_length.append(len(i))\n",
    "    \n",
    "for i in test_x:\n",
    "    test_length.append(len(i))\n",
    "    \n",
    "\n",
    "bins =15\n",
    "hist, bin_edges = np.histogram(train_length, bins=bins)\n",
    "print('train_x length Histogram')\n",
    "print(bin_edges)\n",
    "print(hist)\n",
    "\n",
    "hist, bin_edges = np.histogram(test_length, bins=bins)\n",
    "print('test_x length Histogram')\n",
    "print(bin_edges)\n",
    "print(hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43eedfce-c8cd-485f-946c-d18d3bfaf18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_top_n_percent(data, n=15):\n",
    "    # 각 데이터의 길이 계산\n",
    "    lengths = [len(d) for d in data]\n",
    "    \n",
    "    # 길이 기준으로 내림차순 정렬\n",
    "    sorted_lengths = sorted(lengths, reverse=True)\n",
    "    \n",
    "    # 상위 n% 길이 계산\n",
    "    top_n_percent_length = sorted_lengths[int(len(sorted_lengths) * n / 100)]\n",
    "    \n",
    "    # 길이가 상위 n%에 해당하는 데이터 제거\n",
    "    filtered_data = [d for d in data if len(d) <= top_n_percent_length]\n",
    "    \n",
    "    return np.array(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eca4b2b-e2b8-4f09-a7b5-1258422362a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjb06\\AppData\\Local\\Temp\\ipykernel_3468\\4032520965.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(filtered_data)\n"
     ]
    }
   ],
   "source": [
    "data_type = type(train_x)\n",
    "print(data_type)\n",
    "train_x = remove_top_n_percent(train_x)\n",
    "data_type = type(train_x)\n",
    "print(data_type)\n",
    "train_lengths = [len(d) for d in train_x]\n",
    "test_x = remove_top_n_percent(test_x)\n",
    "test_lengths = [len(d) for d in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d75af281-d65a-4af7-b4c7-a177f1ad3111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Train:\n",
      "89424\n",
      "9352\n",
      "85.01818181818182 %\n",
      "\n",
      "Filtered Test:\n",
      "111136\n",
      "2135\n",
      "85.39999999999999 %\n"
     ]
    }
   ],
   "source": [
    "print('Filtered Train:')\n",
    "print(max(train_lengths))\n",
    "print(len(train_lengths))\n",
    "print(len(train_lengths)/11000*100,'%')\n",
    "print('\\nFiltered Test:')\n",
    "print(max(test_lengths))\n",
    "print(len(test_lengths))\n",
    "print(len(test_lengths)/2500*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b182b40-945d-4330-b284-32842c44b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avr_length(data):\n",
    "    total_length = 0\n",
    "    data_count=0\n",
    "    for i in data:\n",
    "        total_length+=len(i)\n",
    "        data_count+=1\n",
    "    return int(total_length/data_count)\n",
    "    \n",
    "\n",
    "def set_length(data, avr_length):\n",
    "    result = []\n",
    "    for i in tqdm(data,colour='green'):\n",
    "        if len(i) <=avr_length:\n",
    "            padded_data = np.pad(i, (0, avr_length-len(i)), 'wrap')\n",
    "            tmp = padded_data.tolist()\n",
    "            result.append(tmp)   \n",
    "        else:\n",
    "            result.append(i[:avr_length])\n",
    "            \n",
    "    result = np.array(result)\n",
    "    print('데이터 세팅 완료~!')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dc7df52-b651-4c0c-94f8-96471df91537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_type = type(train_x)\n",
    "print(data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc40e3ad-742c-447e-a2b4-77a9543683d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 9352/9352 [00:06<00:00, 1544.32it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 세팅 완료~!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████████████████████████████████████████████████████\u001b[0m| 2135/2135 [00:01<00:00, 1094.81it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 세팅 완료~!\n"
     ]
    }
   ],
   "source": [
    "avr_length=get_avr_length(train_x)\n",
    "train_x = set_length(train_x,avr_length)\n",
    "test_x = set_length(test_x,avr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ee994fc-9412-4dbc-bc28-13c8a62d71ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (9352, 50667)\n",
      "test : (2135, 50667)\n"
     ]
    }
   ],
   "source": [
    "print('train :', train_x.shape)\n",
    "print('test :', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "731056be-4896-46e8-b37e-3c4c3ed3c271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 99)\n",
      "(40, 99)\n"
     ]
    }
   ],
   "source": [
    "extracted_features = librosa.feature.mfcc(y=train_x[0], sr=16000, n_mfcc=40)\n",
    "print(extracted_features.shape)\n",
    "extracted_features = librosa.feature.mfcc(y=test_x[0], sr=16000, n_mfcc=40)\n",
    "print(extracted_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e2a3681-2de7-4445-b594-a9a3c386f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data):\n",
    "    mfccs = []\n",
    "    for i in tqdm(data,colour='green'):\n",
    "        extracted_features = librosa.feature.mfcc(y=i,\n",
    "                                              sr=16000,\n",
    "                                              n_mfcc=40)\n",
    "        mfccs.append(extracted_features)\n",
    "            \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cd79305-0245-4a0c-9973-5395e4d7fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m████████████████████████████████████████████████████████████\u001b[0m| 9352/9352 [01:54<00:00, 81.51it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_mfccs = preprocess_dataset(train_x)\n",
    "train_mfccs = np.array(train_mfccs)\n",
    "train_mfccs = train_mfccs.reshape(-1, train_mfccs.shape[1], train_mfccs.shape[2], 1)\n",
    "#test_x = test_x.reshape(-1, test_x.shape[1], test_x.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "883e7e0f-b714-45db-9372-35d5368334dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9352, 40, 99, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_mfccs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3cdb2-59e7-4451-bfd5-1c90ab423d46",
   "metadata": {},
   "source": [
    "Custom DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "786b7e68-92ae-4cde-9568-8807b4f76e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets # 데이터셋 집합체\n",
    "import torchvision.transforms as transforms # 변환 툴\n",
    "\n",
    "from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y, train_mode=True, transforms=None): #필요한 변수들을 선언\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.train_mode = train_mode\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index): #index번째 data를 return\n",
    "        X = self.X[index]\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            X = self.transforms(X)\n",
    "\n",
    "        if self.train_mode:\n",
    "            y = self.y[index]\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def __len__(self): #길이 return\n",
    "        return len(self.X)\n",
    "\n",
    "train_X = train_mfccs[:8000]\n",
    "vali_X = train_mfccs[8000:]\n",
    "\n",
    "train_y = train_wav.label[:8000]\n",
    "vali_y = train_wav.label[8000:].reset_index(drop = True)\n",
    "\n",
    "# 에포크 설정\n",
    "num_epochs = 100\n",
    "\n",
    "# 배치 사이즈 설정\n",
    "batch_size = 10\n",
    "\n",
    "#만든 train dataset를 DataLoader에 넣어 batch 만들기\n",
    "train_dataset = CustomDataset(X=train_X, y=train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "vali_dataset = CustomDataset(X=vali_X, y=vali_y)\n",
    "vali_loader = DataLoader(vali_dataset, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c35c4eae-6e30-4f88-8fa3-3cdcc9570d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# train_loader와 vali_loader를 dictionary 형태로 저장\n",
    "data = {'train_loader': train_loader, 'vali_loader': vali_loader}\n",
    "\n",
    "# 파일로 저장\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc2d5e40-0092-4f11-b69e-33d9d7f59fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CustomDataset object at 0x000001E9E79517F0>\n",
      "[[[-460.104925  ]\n",
      "  [-423.75440959]\n",
      "  [-407.94510396]\n",
      "  ...\n",
      "  [-548.88746759]\n",
      "  [-548.88746759]\n",
      "  [-548.88746759]]\n",
      "\n",
      " [[  47.59595444]\n",
      "  [  47.56444795]\n",
      "  [  31.10083833]\n",
      "  ...\n",
      "  [   0.        ]\n",
      "  [   0.        ]\n",
      "  [   0.        ]]\n",
      "\n",
      " [[ -49.82005715]\n",
      "  [ -67.93859642]\n",
      "  [ -71.60807103]\n",
      "  ...\n",
      "  [   0.        ]\n",
      "  [   0.        ]\n",
      "  [   0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  -5.39343936]\n",
      "  [  -4.77270195]\n",
      "  [  -7.23844319]\n",
      "  ...\n",
      "  [   0.        ]\n",
      "  [   0.        ]\n",
      "  [   0.        ]]\n",
      "\n",
      " [[  -0.78771144]\n",
      "  [  -2.07211224]\n",
      "  [  -3.82161642]\n",
      "  ...\n",
      "  [   0.        ]\n",
      "  [   0.        ]\n",
      "  [   0.        ]]\n",
      "\n",
      " [[   3.50300228]\n",
      "  [   7.21237114]\n",
      "  [   3.36427638]\n",
      "  ...\n",
      "  [   0.        ]\n",
      "  [   0.        ]\n",
      "  [   0.        ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 99, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "x,y = train_dataset.__getitem__(5)\n",
    "print(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "edfd3185-b262-4666-b2da-d4044837757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ total train batches : 800\n",
      "/ total valid batches : 136\n"
     ]
    }
   ],
   "source": [
    "train_batches = len(train_loader)\n",
    "vali_batches = len(vali_loader)\n",
    "\n",
    "print('/ total train batches :', train_batches)\n",
    "print('/ total valid batches :', vali_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dc4f5ae-4cdf-4512-9c0a-272c20caafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn # 신경망들이 포함됨\n",
    "\n",
    "class CNNclassification(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNclassification, self).__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            nn.Conv2d(40, 10, kernel_size=2, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            nn.Conv2d(10, 100, kernel_size=2, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            nn.Conv2d(100, 200, kernel_size=2, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            nn.Conv2d(200, 300, kernel_size=2, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.layer5 = torch.nn.Sequential(\n",
    "            nn.Conv2d(300, 400, kernel_size=2, stride=1, padding=1), #cnn layer\n",
    "            nn.ReLU(), #activation function\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #pooling layer\n",
    "        \n",
    "        self.fc_layer = nn.Sequential( \n",
    "            nn.Linear(1600, 5) #fully connected layer(ouput layer)\n",
    "        )    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float() # 입력 데이터를 float32 타입으로 변환\n",
    "        x = self.layer1(x) #1층\n",
    "        \n",
    "        x = self.layer2(x) #2층\n",
    "         \n",
    "        x = self.layer3(x) #3층\n",
    "        \n",
    "        x = self.layer4(x) #4층\n",
    "        \n",
    "        x = self.layer5(x) #5층\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1) # N차원 배열 -> 1차원 배열\n",
    "        \n",
    "        out = self.fc_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9c73dec-6442-4db4-8e76-ef6c7f0547da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim # 최적화 알고리즘들이 포함힘\n",
    "\n",
    "model = CNNclassification().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = 1e-3 )\n",
    "scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ac27b0a-9df8-4ced-afd4-2d9abd3d5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0132, -0.0369, -0.0095,  0.0285, -0.0454],\n",
       "        [-0.0135, -0.0366, -0.0094,  0.0292, -0.0455],\n",
       "        [-0.0132, -0.0369, -0.0096,  0.0293, -0.0458],\n",
       "        [-0.0130, -0.0365, -0.0091,  0.0284, -0.0449],\n",
       "        [-0.0132, -0.0372, -0.0092,  0.0287, -0.0452],\n",
       "        [-0.0131, -0.0366, -0.0094,  0.0284, -0.0451],\n",
       "        [-0.0129, -0.0369, -0.0092,  0.0290, -0.0455],\n",
       "        [-0.0138, -0.0371, -0.0093,  0.0281, -0.0460],\n",
       "        [-0.0136, -0.0367, -0.0095,  0.0281, -0.0457],\n",
       "        [-0.0134, -0.0369, -0.0092,  0.0283, -0.0459]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.rand(10,40,99,1).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "868578d0-d807-4eb6-8a29-14c13a08cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model, optimizer, train_loader, scheduler, device): \n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1,num_epochs): #에포크 설정\n",
    "        model.train() #모델 학습\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for wav, label in tqdm(iter(train_loader)):\n",
    "            \n",
    "            wav, label = wav.to(device), label.to(device) #배치 데이터\n",
    "            optimizer.zero_grad() #배치마다 optimizer 초기화\n",
    "        \n",
    "            # Data -> Model -> Output\n",
    "            logit = model(wav) #예측값 산출\n",
    "            loss = criterion(logit, label) #손실함수 계산\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward() #손실함수 기준 역전파 \n",
    "            optimizer.step() #가중치 최적화\n",
    "            running_loss += loss.item()\n",
    "             \n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(train_loader)))\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "        #Validation set 평가\n",
    "        model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
    "        vali_loss = 0.0\n",
    "        correct = 0\n",
    "       \n",
    "        with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
    "            for wav, label in tqdm(iter(vali_loader)):\n",
    "                \n",
    "                wav, label = wav.to(device), label.to(device)\n",
    "                logit = model(wav)\n",
    "                vali_loss += criterion(logit, label)\n",
    "                pred = logit.argmax(dim=1, keepdim=True)  #10개의 class중 가장 값이 높은 것을 예측 label로 추출\n",
    "                correct += pred.eq(label.view_as(pred)).sum().item() #예측값과 실제값이 맞으면 1 아니면 0으로 합산\n",
    "        vali_acc = 100 * correct / len(vali_loader.dataset)\n",
    "        print('Vail set: Loss: {:.4f}, Accuracy: {}/{} ( {:.0f}%)\\n'.format(vali_loss / len(vali_loader), correct, len(vali_loader.dataset), 100 * correct / len(vali_loader.dataset)))\n",
    "        \n",
    "        #베스트 모델 저장\n",
    "        if best_acc < vali_acc:\n",
    "            best_acc = vali_acc\n",
    "            torch.save(model.state_dict(), 'data/saved/best_model2.pth') #이 디렉토리에 best_model.pth을 저장\n",
    "            print('Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71a16cb9-d96a-4610-81dc-4d8be200d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjb06\\AppData\\Local\\Temp\\ipykernel_3468\\2892129855.py:2: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2d371de9274c4e80d87bc44bf8caff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3468\\2892129855.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TORCH_USE_CUDA_DSA'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3468\\374822955.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mwav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mwav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#배치 데이터\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#배치마다 optimizer 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "with torch.autograd.detect_anomaly():\n",
    "    train(model, optimizer, train_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78408da0-eebb-44d2-a108-87dc42f0fd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
